# Psychology Agent
A LLM-based mental health assistant integrating **Agent architecture** and **RLHF (Reinforcement Learning from Human Feedback)**, continuously optimized through multimodal data analysis, personality-based personalization, and human feedback.

## LLM Method Improvements & Technical Innovations



## Architecture

### Core Components

```
User Input
  â†“
1. Mental State Interpreter â†’ Understand user's emotional state
  â†“
2. Personality Analyzer â†’ Analyze personality traits (after 3+ conversations)
  â†“
3. RLHF Reward Model â†’ Get personalized weights (based on personality)
  â†“
4. LLM-Coach â†’ Generate initial response
  â†“
5. Critic Agent â†’ Evaluate response quality
  â†“
6. Refiner Agent â†’ Optimize response (if needed)
  â†“
Final Response (High-quality, Personalized)
```

### Key Modules

1. **Mental State Interpreter**: Analyzes user's current psychological state
2. **Personality Analyzer**: Extracts Big Five traits and preferences from conversations
3. **RLHF Reward Model**: Personalized reward weights based on personality + feedback
4. **LLM-Coach**: Generates CBT/ACT-style therapeutic responses
5. **Critic Agent**: Quality assessment and routing decisions
6. **Refiner Agent**: Iterative response optimization

## Key Features Explained

### Quality Assurance Pipeline

Every response goes through:
1. **Initial Generation**: LLM-Coach generates response
2. **Quality Assessment**: Critic Agent evaluates quality
3. **Smart Routing**: 
   - High quality â†’ Use directly
   - Medium quality â†’ Refine
   - Low quality â†’ Regenerate with multiple candidates
4. **Final Response**: Optimized, personalized, high-quality


### Personality-Based RLHF

The system automatically analyzes user personality after 3+ conversations and initializes personalized reward weights:

- **High Neuroticism** â†’ More emotional stability support
- **Introverted** â†’ More self-efficacy reinforcement
- **Problem-focused** â†’ More practical advice
- **Emotion-focused** â†’ More compassion and support




### Multi-Agent LLM Architecture Improvements

This system implements a multi-agent architecture following the **Record-Understand-Decide-Execute** paradigm:

**1. Record (Memory)**: Graph-Based State & Reasoning History
- Stores conversation history and mental state transitions
- Tracks user personality profile and preferences
- Maintains long-term memory for personalized responses

**2. Understand (Reasoning Core)**: Reasoner Agent (LLM-powered)
- Mental State Interpreter: Analyzes user's psychological state
- Personality Analyzer: Extracts personality traits from conversations
- Multi-step reasoning to understand user needs and context

**3. Decide (Controller)**: Critic Agent (Confidence-based routing)
- Evaluates response quality across 6 dimensions
- Makes routing decisions: use / refine / regenerate
- Confidence-based decision making for optimal response selection

**4. Execute (Tool Layer)**: Refiner Agent (Answer synthesis)
- Synthesizes final response based on Critic feedback
- Generates multiple candidate responses with different strategies
- Iteratively optimizes response quality

**Workflow:**
```
User Input
  â†’ Record: Store in memory & retrieve context
  â†’ Understand: Mental State + Personality Analysis (Reasoner)
  â†’ Decide: Quality Assessment & Routing (Critic)
  â†’ Execute: Response Generation & Refinement (Refiner)
  â†’ Final Response
```

 



## Project Structure

```
Agent_psychology_assistant/
â”œâ”€â”€ agent/                          # Agent core modules
â”‚   â”œâ”€â”€ conversation_manager.py     # Main conversation orchestrator
â”‚   â”œâ”€â”€ mental_state_interpreter.py # Psychological state analysis
â”‚   â”œâ”€â”€ llm_coach.py                # CBT/ACT response generation
â”‚   â”œâ”€â”€ personality_analyzer.py     # Personality analysis
â”‚   â”œâ”€â”€ critic_agent.py            # Quality assessment
â”‚   â”œâ”€â”€ refiner_agent.py            # Response optimization
â”‚   â””â”€â”€ memory_system.py            # User profile & memory
â”œâ”€â”€ models/                         # LLM wrapper
â”‚   â”œâ”€â”€ llm_configs.py              # Model configuration
â”‚   â””â”€â”€ llm_orchestrator.py         # Unified LLM API
â”œâ”€â”€ rlhf/                           # RLHF modules
â”‚   â”œâ”€â”€ personalized_reward_model.py # Personality-based RLHF
â”‚   â”œâ”€â”€ reward_model.py             # Base reward model
â”‚   â””â”€â”€ feedback_collector.py       # Feedback collection
â”œâ”€â”€ safety/                         # Safety module
â”‚   â””â”€â”€ crisis_detection.py         # Crisis detection
â”œâ”€â”€ llm_analysis/                   # Analysis modules
â”‚   â””â”€â”€ behavior_analyzer.py        # Behavior pattern analysis
â”œâ”€â”€ data_collection/                # Data collection
â”‚   â””â”€â”€ search_log_processor.py    # Search log processing
â”œâ”€â”€ main.py                         # Main application entry
â””â”€â”€ requirements.txt                # Dependencies
```

## Quick Start

### 1. Clone Repository

```bash
git clone https://github.com/kevinlmf/Psychology_Agent
cd Psychology_Agent

```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure API Keys

**Important: Never commit your `.env` file to git!**

Create a `.env` file and add your API keys:

```bash
# Create .env file
cat > .env << EOF
ANTHROPIC_API_KEY=sk-ant-api03-your-actual-key-here
# Optional:
# OPENAI_API_KEY=sk-your-openai-key-here
EOF
```

Or manually create `.env`:

```env
ANTHROPIC_API_KEY=sk-ant-api03-your-actual-key-here
```

Get your Anthropic API key from: https://console.anthropic.com/settings/keys


### 4. Run Application

```bash
python main.py
```

Select mode:
- **Mode 1**: Interactive conversation (full experience with enhanced architecture)
- **Mode 2**: Demo basic conversation features
- **Mode 3**: Demo behavior analysis
- **Mode 4**: Demo crisis detection
- **Mode 5**: Demo how to be happier




## Configuration

### Model Configuration

Models are configured in `models/llm_configs.py`. Currently using:
- `claude-3-7-sonnet-20250219` for most tasks
- Configurable per task type (crisis detection, casual chat, behavior analysis, etc.)

### RLHF Configuration

RLHF weights are personalized per user and stored in:
- `psychology_agent/data/rlhf/{user_id}_reward_weights.json`

### Personality Analysis

Personality profiles are stored in user profiles:
- `psychology_agent/data/user_profiles/{user_id}_profile.json`
---
##  Disclaimer

This system is for **research and educational purposes only**.  
It is **not** a medical device and must not be used for diagnosis or treatment.  
Consult professionals for real medical issues.
---
Happy Happy HappyðŸ˜Š
